{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing as pp\n",
    "import implementations as imp\n",
    "import helpers as hlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/maelynenguyen/Desktop/\"\n",
    "\n",
    "x_train_ = np.load(path+\"f_x_train_.npy\")\n",
    "x_test_ = np.load(path+\"f_x_test_.npy\")\n",
    "y_train_ = np.load(path+\"f_y_train_.npy\")\n",
    "test_ids = np.load(path+\"f_test_ids_.npy\")\n",
    "train_ids = np.load(path+\"f_train_ids_.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max median NaN score rows :  0.5688708393430034\n",
      "Max median NaN score columns :  0.7598051755967697\n",
      "Number of rows dropped because of a NaN score > 0.5:  29455\n",
      "Number of columns dropped because of a NaN score > 0.5:  149\n",
      "Number of columns with std < 0.1: 5\n",
      "Number of columns with correl_coef > 0.95: 19\n",
      "Handling NaN values...\n",
      "Number of columns with corr_coef> 0.95 after cleaning: 2\n",
      "The data has been cleaned and standardized\n",
      "The cleaned x-data has the following shape:  (298680, 146)\n",
      "The cleaned y-data has the following shape:  (298680,)\n",
      "The cleaned x-data-to-predict has the following shape:  (109379, 146)\n"
     ]
    }
   ],
   "source": [
    "x, x_submit, y = pp.Edited_clean_data(x_train_,  y_train_, x_test_)\n",
    "\n",
    "#important to add a constant term for the bias\n",
    "x_train = np.concatenate((x,np.zeros((x.shape[0],1))+1),axis=1)\n",
    "x_test = np.concatenate((x_submit,np.zeros((x_submit.shape[0],1))+1),axis=1)\n",
    "\n",
    "assert x.shape[1]+1 == x_train.shape[1]\n",
    "\n",
    "y_train = y.copy()\n",
    "y_train = np.where(y == -1, 0, 1)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "#X,Y,X_T,Y_T = pp.split_data(x,y_train ,.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Cross Validation in a Grid Search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation x Grid Search for a logistic regression\n",
    "def k_fold_split(x, y, k):\n",
    "    \"\"\"Utility function to split data into k folds.\"\"\"\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = len(y) // k\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.concatenate((indices[:i * fold_size], indices[(i + 1) * fold_size:]))\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds\n",
    "    \n",
    "def grid_search_logistic_regression(y_train, x_train_cleaned, param_grid, w_initial, k=5):\n",
    "    best_params = None\n",
    "    best_score = float(\"inf\")\n",
    "    best_w = w_initial\n",
    "    losses = []\n",
    "        \n",
    "        # Generate folds for cross-validation\n",
    "    folds = k_fold_split(x_train_cleaned, y_train, k)\n",
    "\n",
    "        # Iterate over each combination of parameters\n",
    "    for max_iters in param_grid[\"max_iters\"]:\n",
    "        for gamma in param_grid[\"gamma\"]:\n",
    "            total_loss = 0\n",
    "                \n",
    "                # Perform k-fold cross-validation\n",
    "            for train_indices, test_indices in folds:\n",
    "                x_train_fold = x_train_cleaned[train_indices]\n",
    "                y_train_fold = y_train[train_indices]\n",
    "                x_test_fold = x_train_cleaned[test_indices]\n",
    "                y_test_fold = y_train[test_indices]\n",
    "                    \n",
    "                w, loss = imp.logistic_regression(y_train_fold, x_train_fold, w_initial, max_iters, gamma)\n",
    "                total_loss += loss\n",
    "\n",
    "            avg_loss = total_loss / k\n",
    "            losses.append((max_iters, gamma, avg_loss))\n",
    "            print(f\"Max Iters: {max_iters}, Gamma: {gamma}, Avg Loss: {avg_loss}\")\n",
    "\n",
    "            if avg_loss < best_score:\n",
    "                best_score = avg_loss\n",
    "                best_w = w\n",
    "                best_params = {\n",
    "                    \"max_iters\": max_iters,\n",
    "                    \"gamma\": gamma,\n",
    "                }\n",
    "\n",
    "    return best_w, best_params, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pris de chat mais à modifier si vous voyez des trucs à changer \n",
    "def f1_score_(y_true, y_pred):\n",
    "    # True Positives (TP): Both predicted and actual are positive (1)\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    \n",
    "    # False Positives (FP): Predicted positive (1) but actual negative (0)\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    # False Negatives (FN): Predicted negative (0) but actual positive (1)\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Precision: TP / (TP + FP)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    \n",
    "    # Recall: TP / (TP + FN)\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold(y_pred,y):\n",
    "    # Search for the best threshold\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    f1_scores = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thres = np.where(y_pred > threshold, 1, -1)\n",
    "        f1 = f1_score_(y, y_pred_thres)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "# Find the threshold with the highest F1 score\n",
    "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_iters\": [1500],\n",
    "    \"gamma\": [1,0.1, 0.001],\n",
    "    \"lambda_\": [1e-4,1e-5,1e-6],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Iters: 1500, Gamma: 1, Avg Loss: 0.2293439389906225\n",
      "Max Iters: 1500, Gamma: 0.1, Avg Loss: 0.22991550387791732\n",
      "Max Iters: 1500, Gamma: 0.001, Avg Loss: 0.4828176750376191\n",
      "Best hyperparameters found:  {'max_iters': 1500, 'gamma': 1}\n",
      "[0.02266315 0.02811419 0.01355373 ... 0.08385015 0.02119777 0.02104295]\n",
      "0.81\n"
     ]
    }
   ],
   "source": [
    "w_grid, best_params, losses = grid_search_logistic_regression(y_train, x_train, param_grid, np.zeros(x_train.shape[1]))\n",
    "print(\"Best hyperparameters found: \", best_params)\n",
    "\n",
    "y_train_grid = imp.sigmoid(np.dot(x_train,w_grid))\n",
    "optimal_threshold = best_threshold(y_train_grid,y_train)\n",
    "print(y_train_grid)\n",
    "y_pred = imp.sigmoid(np.dot(x_test, w_grid))\n",
    "y_pred = np.where(y_pred > optimal_threshold , 1, -1)\n",
    "print(optimal_threshold)\n",
    "hlp.create_csv_submission(test_ids, y_pred, \"y_pred_grid_retest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of an Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam optimizer \n",
    "def adam_optimizer(y, x, w, max_iters, gamma, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    m, v = np.zeros_like(w), np.zeros_like(w)\n",
    "    for iter in range(max_iters):\n",
    "        # Compute predictions and gradients\n",
    "        \n",
    "        predictions = imp.sigmoid(np.dot(x, w))\n",
    "        gradient = np.dot(x.T, predictions - y)\n",
    "\n",
    "        m = beta1 * m + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * (gradient ** 2)\n",
    "\n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1 ** (iter + 1))\n",
    "        v_hat = v / (1 - beta2 ** (iter + 1))\n",
    "\n",
    "        # Update weights\n",
    "        w -= gamma * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_adam = adam_optimizer(y_train, x_train, np.zeros(x_train.shape[1]), 1500, 0.1)\n",
    "\n",
    "y_train_adam = imp.sigmoid(np.dot(x_train,w_adam))\n",
    "optimal_threshold = best_threshold(y_train_adam,y_train)\n",
    "\n",
    "y_pred_adam = imp.sigmoid(np.dot(x_test, w_adam))\n",
    "y_pred_adam  = np.where(y_pred_adam > optimal_threshold , 1, -1)\n",
    "\n",
    "hlp.create_csv_submission(test_ids, y_pred_adam, \"y_pred_adam_old.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
