{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing as pp\n",
    "import implementations as imp\n",
    "import helpers as hlp\n",
    "import os\n",
    "from functions import *\n",
    "import index as idx \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/maelynenguyen/Desktop/\"\n",
    "\n",
    "x_train_ = np.load(path+\"f_x_train_.npy\")\n",
    "x_test_ = np.load(path+\"f_x_test_.npy\")\n",
    "y_train_ = np.load(path+\"f_y_train_.npy\")\n",
    "test_ids = np.load(path+\"f_test_ids_.npy\")\n",
    "train_ids = np.load(path+\"f_train_ids_.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = '/Users/maelynenguyen/Desktop/dataset_to_release'\n",
    "\n",
    "x_train_t, x_test_t, y_train_t, train_ids_, test_ids_ = hlp.load_csv_data(path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in x_train.csv: ['Id', '_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU', 'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES', 'CELLFON3', 'LADULT', 'NUMADULT', 'NUMMEN', 'NUMWOMEN', 'CTELNUM1', 'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS', 'CSTATE', 'LANDLINE', 'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH', 'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY', 'DIABETE3', 'DIABAGE2', 'SEX', 'MARITAL', 'EDUCA', 'RENTHOM1', 'NUMHHOL2', 'NUMPHON2', 'CPDEMO1', 'VETERAN3', 'EMPLOY1', 'CHILDREN', 'INCOME2', 'INTERNET', 'WEIGHT2', 'HEIGHT3', 'PREGNANT', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'SMOKDAY2', 'STOPSMK2', 'LASTSMK2', 'USENOW3', 'ALCDAY5', 'AVEDRNK2', 'DRNK3GE5', 'MAXDRNKS', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN', 'FVORANG', 'VEGETAB1', 'EXERANY2', 'EXRACT11', 'EXEROFT1', 'EXERHMM1', 'EXRACT21', 'EXEROFT2', 'EXERHMM2', 'STRENGTH', 'LMTJOIN3', 'ARTHDIS2', 'ARTHSOCL', 'JOINPAIN', 'SEATBELT', 'FLUSHOT6', 'FLSHTMY2', 'IMFVPLAC', 'PNEUVAC3', 'HIVTST6', 'HIVTSTD3', 'WHRTST10', 'PDIABTST', 'PREDIAB1', 'INSULIN', 'BLDSUGAR', 'FEETCHK2', 'DOCTDIAB', 'CHKHEMO3', 'FEETCHK', 'EYEEXAM', 'DIABEYE', 'DIABEDU', 'CAREGIV1', 'CRGVREL1', 'CRGVLNG1', 'CRGVHRS1', 'CRGVPRB1', 'CRGVPERS', 'CRGVHOUS', 'CRGVMST2', 'CRGVEXPT', 'VIDFCLT2', 'VIREDIF3', 'VIPRFVS2', 'VINOCRE2', 'VIEYEXM2', 'VIINSUR2', 'VICTRCT4', 'VIGLUMA2', 'VIMACDG2', 'CIMEMLOS', 'CDHOUSE', 'CDASSIST', 'CDHELP', 'CDSOCIAL', 'CDDISCUS', 'WTCHSALT', 'LONGWTCH', 'DRADVISE', 'ASTHMAGE', 'ASATTACK', 'ASERVIST', 'ASDRVIST', 'ASRCHKUP', 'ASACTLIM', 'ASYMPTOM', 'ASNOSLEP', 'ASTHMED3', 'ASINHALR', 'HAREHAB1', 'STREHAB1', 'CVDASPRN', 'ASPUNSAF', 'RLIVPAIN', 'RDUCHART', 'RDUCSTRK', 'ARTTODAY', 'ARTHWGT', 'ARTHEXER', 'ARTHEDU', 'TETANUS', 'HPVADVC2', 'HPVADSHT', 'SHINGLE2', 'HADMAM', 'HOWLONG', 'HADPAP2', 'LASTPAP2', 'HPVTEST', 'HPLSTTST', 'HADHYST2', 'PROFEXAM', 'LENGEXAM', 'BLDSTOOL', 'LSTBLDS3', 'HADSIGM3', 'HADSGCO1', 'LASTSIG3', 'PCPSAAD2', 'PCPSADI1', 'PCPSARE1', 'PSATEST1', 'PSATIME', 'PCPSARS1', 'PCPSADE1', 'PCDMDECN', 'SCNTMNY1', 'SCNTMEL1', 'SCNTPAID', 'SCNTWRK1', 'SCNTLPAD', 'SCNTLWK1', 'SXORIENT', 'TRNSGNDR', 'RCSGENDR', 'RCSRLTN2', 'CASTHDX2', 'CASTHNO2', 'EMTSUPRT', 'LSATISFY', 'ADPLEASR', 'ADDOWN', 'ADSLEEP', 'ADENERGY', 'ADEAT1', 'ADFAIL', 'ADTHINK', 'ADMOVE', 'MISTMNT', 'ADANXEV', 'QSTVER', 'QSTLANG', 'MSCODE', '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_CHISPNC', '_CRACE1', '_CPRACE', '_CLLCPWT', '_DUALUSE', '_DUALCOR', '_LLCPWT', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1', '_MRACE1', '_HISPANC', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', 'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', 'DROCDY3_', '_RFBING5', '_DRNKWEK', '_RFDRHV5', 'FTJUDA1_', 'FRUTDA1_', 'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN', '_MISVEGN', '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_VEGESUM', '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX', '_TOTINDA', 'METVL11_', 'METVL21_', 'MAXVO2_', 'FC60_', 'ACTIN11_', 'ACTIN21_', 'PADUR1_', 'PADUR2_', 'PAFREQ1_', 'PAFREQ2_', '_MINAC11', '_MINAC21', 'STRFREQ_', 'PAMISS1_', 'PAMIN11_', 'PAMIN21_', 'PA1MIN_', 'PAVIG11_', 'PAVIG21_', 'PA1VIGM_', '_PACAT1', '_PAINDX1', '_PA150R2', '_PA300R2', '_PA30021', '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_FLSHOT6', '_PNEUMO2', '_AIDTST3']\n",
      "Features in x_test.csv: ['Id', '_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU', 'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES', 'CELLFON3', 'LADULT', 'NUMADULT', 'NUMMEN', 'NUMWOMEN', 'CTELNUM1', 'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS', 'CSTATE', 'LANDLINE', 'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH', 'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY', 'DIABETE3', 'DIABAGE2', 'SEX', 'MARITAL', 'EDUCA', 'RENTHOM1', 'NUMHHOL2', 'NUMPHON2', 'CPDEMO1', 'VETERAN3', 'EMPLOY1', 'CHILDREN', 'INCOME2', 'INTERNET', 'WEIGHT2', 'HEIGHT3', 'PREGNANT', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'SMOKDAY2', 'STOPSMK2', 'LASTSMK2', 'USENOW3', 'ALCDAY5', 'AVEDRNK2', 'DRNK3GE5', 'MAXDRNKS', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN', 'FVORANG', 'VEGETAB1', 'EXERANY2', 'EXRACT11', 'EXEROFT1', 'EXERHMM1', 'EXRACT21', 'EXEROFT2', 'EXERHMM2', 'STRENGTH', 'LMTJOIN3', 'ARTHDIS2', 'ARTHSOCL', 'JOINPAIN', 'SEATBELT', 'FLUSHOT6', 'FLSHTMY2', 'IMFVPLAC', 'PNEUVAC3', 'HIVTST6', 'HIVTSTD3', 'WHRTST10', 'PDIABTST', 'PREDIAB1', 'INSULIN', 'BLDSUGAR', 'FEETCHK2', 'DOCTDIAB', 'CHKHEMO3', 'FEETCHK', 'EYEEXAM', 'DIABEYE', 'DIABEDU', 'CAREGIV1', 'CRGVREL1', 'CRGVLNG1', 'CRGVHRS1', 'CRGVPRB1', 'CRGVPERS', 'CRGVHOUS', 'CRGVMST2', 'CRGVEXPT', 'VIDFCLT2', 'VIREDIF3', 'VIPRFVS2', 'VINOCRE2', 'VIEYEXM2', 'VIINSUR2', 'VICTRCT4', 'VIGLUMA2', 'VIMACDG2', 'CIMEMLOS', 'CDHOUSE', 'CDASSIST', 'CDHELP', 'CDSOCIAL', 'CDDISCUS', 'WTCHSALT', 'LONGWTCH', 'DRADVISE', 'ASTHMAGE', 'ASATTACK', 'ASERVIST', 'ASDRVIST', 'ASRCHKUP', 'ASACTLIM', 'ASYMPTOM', 'ASNOSLEP', 'ASTHMED3', 'ASINHALR', 'HAREHAB1', 'STREHAB1', 'CVDASPRN', 'ASPUNSAF', 'RLIVPAIN', 'RDUCHART', 'RDUCSTRK', 'ARTTODAY', 'ARTHWGT', 'ARTHEXER', 'ARTHEDU', 'TETANUS', 'HPVADVC2', 'HPVADSHT', 'SHINGLE2', 'HADMAM', 'HOWLONG', 'HADPAP2', 'LASTPAP2', 'HPVTEST', 'HPLSTTST', 'HADHYST2', 'PROFEXAM', 'LENGEXAM', 'BLDSTOOL', 'LSTBLDS3', 'HADSIGM3', 'HADSGCO1', 'LASTSIG3', 'PCPSAAD2', 'PCPSADI1', 'PCPSARE1', 'PSATEST1', 'PSATIME', 'PCPSARS1', 'PCPSADE1', 'PCDMDECN', 'SCNTMNY1', 'SCNTMEL1', 'SCNTPAID', 'SCNTWRK1', 'SCNTLPAD', 'SCNTLWK1', 'SXORIENT', 'TRNSGNDR', 'RCSGENDR', 'RCSRLTN2', 'CASTHDX2', 'CASTHNO2', 'EMTSUPRT', 'LSATISFY', 'ADPLEASR', 'ADDOWN', 'ADSLEEP', 'ADENERGY', 'ADEAT1', 'ADFAIL', 'ADTHINK', 'ADMOVE', 'MISTMNT', 'ADANXEV', 'QSTVER', 'QSTLANG', 'MSCODE', '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_CHISPNC', '_CRACE1', '_CPRACE', '_CLLCPWT', '_DUALUSE', '_DUALCOR', '_LLCPWT', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1', '_MRACE1', '_HISPANC', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', 'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', 'DROCDY3_', '_RFBING5', '_DRNKWEK', '_RFDRHV5', 'FTJUDA1_', 'FRUTDA1_', 'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN', '_MISVEGN', '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_VEGESUM', '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX', '_TOTINDA', 'METVL11_', 'METVL21_', 'MAXVO2_', 'FC60_', 'ACTIN11_', 'ACTIN21_', 'PADUR1_', 'PADUR2_', 'PAFREQ1_', 'PAFREQ2_', '_MINAC11', '_MINAC21', 'STRFREQ_', 'PAMISS1_', 'PAMIN11_', 'PAMIN21_', 'PA1MIN_', 'PAVIG11_', 'PAVIG21_', 'PA1VIGM_', '_PACAT1', '_PAINDX1', '_PA150R2', '_PA300R2', '_PA30021', '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_FLSHOT6', '_PNEUMO2', '_AIDTST3']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Load the CSV files and extract the feature names\n",
    "with open('/Users/maelynenguyen/Desktop/dataset_to_release/x_train.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    x_train_features = next(reader)\n",
    "\n",
    "with open('/Users/maelynenguyen/Desktop/dataset_to_release/x_test.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    x_test_features = next(reader)\n",
    "\n",
    "print(\"Features in x_train.csv:\", x_train_features)\n",
    "print(\"Features in x_test.csv:\", x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_9 = idx.index_remplace_9(x_train_features)\n",
    "index_7 = idx.index_remplace_7(x_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_, x_test_ = idx.remplace(index_9,x_train_,x_test_)\n",
    "x_train_, x_test_ = idx.remplace(index_7,x_train_,x_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max median NaN score rows :  0.6060963867246555\n",
      "Max median NaN score columns :  0.7652518774030354\n",
      "Number of rows dropped because of a NaN score > 0.5:  37081\n",
      "Number of columns dropped because of a NaN score > 0.5:  150\n",
      "Number of columns with std < 0.1: 5\n",
      "Number of columns with correl_coef > 0.95: 11\n",
      "Handling NaN values...\n",
      "Data clipped between 5th and 95th percentiles\n",
      "Number of columns with std < 0.1 after cleaning: 11\n",
      "Number of columns with corr_coef> 0.95 after cleaning: 14\n",
      "The data has been cleaned and standardized\n",
      "The cleaned x-data has the following shape:  (291054, 130)\n",
      "The cleaned y-data has the following shape:  (291054,)\n",
      "The cleaned x-data-to-predict has the following shape:  (109379, 130)\n"
     ]
    }
   ],
   "source": [
    "x, x_submit, y = pp.clean_data_final(x_train_,  y_train_t, x_test_)\n",
    "\n",
    "# important to add a constant term for the bias\n",
    "x_train = np.concatenate((x,np.zeros((x.shape[0],1))+1),axis=1)\n",
    "x_test = np.concatenate((x_submit,np.zeros((x_submit.shape[0],1))+1),axis=1)\n",
    "\n",
    "\n",
    "assert x.shape[1]+1 == x_train.shape[1]\n",
    "\n",
    "y_train = y.copy()\n",
    "y_train = np.where(y == -1, 0, 1)  # change -1 to 0\n",
    "y_train = y_train.astype(int)      # change to int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss, losses = imp.logistic_regression(y_train, x_train, np.zeros(x_train.shape[1]), 1500, 0.1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = imp.sigmoid(x_train@w)\n",
    "thr = best_threshold(y_pred,y) # y is (-1,1) and y_pred is (0,1)\n",
    "y_pred_ = np.where(y_pred > thr, 1, -1) # y_pred_ is (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41700883230437485"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_(y, y_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Cross Validation in a Grid Search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_iters\": [1500],\n",
    "    \"gamma\": [1,0.1, 0.001],\n",
    "    \"lambda_\": [1e-4,1e-5,1e-6],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Iters: 1500, Gamma: 1, Avg Loss: 0.22977808706011765\n",
      "Max Iters: 1500, Gamma: 0.1, Avg Loss: 0.22991349322823157\n",
      "Max Iters: 1500, Gamma: 0.001, Avg Loss: 0.4900182968043444\n"
     ]
    }
   ],
   "source": [
    "w_grid, best_params, losses = grid_search_logistic_regression(y_train, x_train, param_grid, np.zeros(x_train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:  {'max_iters': 1500, 'gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold found:  0.19\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = imp.sigmoid(x_train@w_grid) # y_train_pred is (0,1)\n",
    "optimal_threshold = best_threshold(y_train_pred,y) \n",
    "print(\"Optimal threshold found: \", optimal_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = imp.sigmoid(x_test@w_grid)\n",
    "y_pred_ = np.where(y_pred > optimal_threshold, 1, -1)\n",
    "hlp.create_csv_submission(test_ids_, y_pred_, \"logistic_regression24102147.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of an Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_adam = adam_optimizer(y_train, x_train, np.zeros(x_train.shape[1]), 1500, 0.1)\n",
    "\n",
    "y_train_adam = imp.sigmoid(np.dot(x_train, w_adam)) \n",
    "optimal_threshold = best_threshold(y_train_adam, y) # y_train is (-1,1) and y_train_adam is (0,1)\n",
    "\n",
    "y_pred_adam = imp.sigmoid(np.dot(x_test, w_adam))\n",
    "y_pred_adam  = np.where(y_pred_adam > optimal_threshold , 1, -1)\n",
    "\n",
    "hlp.create_csv_submission(test_ids_, y_pred_adam, \"adam_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_split(x, y, k):\n",
    "    \"\"\"Utility function to split data into k folds.\"\"\"\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = len(y) // k\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.concatenate((indices[:i * fold_size], indices[(i + 1) * fold_size:]))\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_reg_logistic_regression(y_train, x_train_cleaned, param_grid, w_initial, k=5):\n",
    "    best_params = None\n",
    "    best_score = float(\"inf\")\n",
    "    best_w = w_initial\n",
    "    losses = []\n",
    "        \n",
    "    # Generate folds for cross-validation\n",
    "    folds = k_fold_split(x_train_cleaned, y_train, k)\n",
    "\n",
    "    # Iterate over each combination of parameters\n",
    "    for max_iters in param_grid[\"max_iters\"]:\n",
    "        for gamma in param_grid[\"gamma\"]:\n",
    "            for lambda_ in param_grid[\"lambda_\"]:\n",
    "                total_loss = 0    \n",
    "                # Perform k-fold cross-validation\n",
    "                for train_indices, test_indices in folds:\n",
    "                    x_train_fold = x_train_cleaned[train_indices]\n",
    "                    y_train_fold = y_train[train_indices]\n",
    "                    x_test_fold = x_train_cleaned[test_indices]\n",
    "                    y_test_fold = y_train[test_indices]\n",
    "                    \n",
    "                    # Train the model with training data\n",
    "                    w, _ = imp.reg_logistic_regression(y_train_fold, x_train_fold, lambda_, w_initial, max_iters, gamma)\n",
    "                    # Test the model with testing data\n",
    "                    loss = imp.logistic_loss(y_test_fold, x_test_fold, w)\n",
    "                    \n",
    "                    total_loss += loss\n",
    "\n",
    "                avg_loss = total_loss / k\n",
    "                losses.append((max_iters, gamma, lambda_, avg_loss))\n",
    "                print(f\"Max Iters: {max_iters}, Gamma: {gamma}, Lambda: {lambda_}, Avg Loss: {avg_loss}\")\n",
    "\n",
    "                if avg_loss < best_score:\n",
    "                    best_score = avg_loss\n",
    "                    best_w = w\n",
    "                    best_params = {\n",
    "                        \"max_iters\": max_iters,\n",
    "                        \"gamma\": gamma,\n",
    "                        \"lambda_\": lambda_,\n",
    "                    }\n",
    "\n",
    "    return best_w, best_params, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Iters: 1500, Gamma: 1, Lambda: 0.0001, Avg Loss: 0.2297606158482602\n",
      "Max Iters: 1500, Gamma: 1, Lambda: 1e-05, Avg Loss: 0.22976041756546514\n",
      "Max Iters: 1500, Gamma: 1, Lambda: 1e-06, Avg Loss: 0.2297616920874618\n",
      "Max Iters: 1500, Gamma: 0.1, Lambda: 0.0001, Avg Loss: 0.22993585946287043\n",
      "Max Iters: 1500, Gamma: 0.1, Lambda: 1e-05, Avg Loss: 0.2298919415847535\n",
      "Max Iters: 1500, Gamma: 0.1, Lambda: 1e-06, Avg Loss: 0.22988811346281293\n",
      "Max Iters: 1500, Gamma: 0.001, Lambda: 0.0001, Avg Loss: 0.4900305808722318\n",
      "Max Iters: 1500, Gamma: 0.001, Lambda: 1e-05, Avg Loss: 0.4900109562359458\n",
      "Max Iters: 1500, Gamma: 0.001, Lambda: 1e-06, Avg Loss: 0.49000899364689304\n"
     ]
    }
   ],
   "source": [
    "w_reg, best_params, losses = grid_search_reg_logistic_regression(y_train, x_train, param_grid, np.zeros(x_train.shape[1]))\n",
    "#w_reg, loss_reg = imp.reg_logistic_regression(y_train, x_train, 1e-6, np.zeros(x_train.shape[1]), 1500, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:  {'max_iters': 1500, 'gamma': 1, 'lambda_': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold found for the regularized model:  0.19\n"
     ]
    }
   ],
   "source": [
    "y_train_reg = imp.sigmoid(np.dot(x_train, w_reg))\n",
    "optimal_threshold_reg = best_threshold(y_train_reg, y) # y_train is (-1,1) and y_train_reg is (0,1)\n",
    "print(\"Optimal threshold found for the regularized model: \", optimal_threshold_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reg = imp.sigmoid(np.dot(x_test, w_reg))\n",
    "y_pred_reg = np.where(y_pred_reg > optimal_threshold_reg, 1, -1)\n",
    "\n",
    "hlp.create_csv_submission(test_ids_, y_pred_reg, \"y_pred_reg27101206.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
